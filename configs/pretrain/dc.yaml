trinary_mz:
  test: 123

  # Training hparams
  batch_size: 100
  ref_batch_size: 100
  blr: 4.0e-5
  weight_decay: 1.0e-5
  epochs: 1
  subset: 0
  num_workers: 0
  dropout: 0

  # Annealing
  lr_warmup: 0
  lr_decay: 0

masked:
  # Masked trainer specific hparams
  predict_fourier: True
  mask_token_fourier: True
  learned_masked_token: True
  positional_encoding: True
  mz_to_int_ratio: 0.67
  mask_ratio: 0.75
  
  # Training hparams
  batch_size: 100
  ref_batch_size: 100
  blr: 4.0e-5
  weight_decay: 1.0e-5
  epochs: 1
  subset: 0
  num_workers: 0
  dropout: 0

  # Annealing
  lr_warmup: 0
  lr_decay: 0

masked_ae:
  # Masked Autoencoder trainer specific hparams
  predict_fourier: True
  mask_token_fourier: True
  learned_masked_token: True
  positional_encoding: True
  mz_to_int_ratio: 0.67
  mask_ratio: 0.75

  # Scaling for regression target (non-fourier target)
  mz_scale_mean: 448.74
  mz_scale_std: 223.25

  # Decoder architecture hparams
  decoder_running_units: 256
  decoder_nhead: 8
  decoder_dim_feedforward: 1024
  decoder_dropout: 0
  decoder_nlayers: 6

  # Training hparams
  batch_size: 100
  ref_batch_size: 100
  blr: 4.0e-5
  weight_decay: 1.0e-5
  epochs: 1
  subset: 0
  num_workers: 0
  dropout: 0

  # Annealing
  lr_warmup: 0
  lr_decay: 0


dino:
  # Data augmentation
  rand_window_size: False
  global_crops_scale:
    - 0.5
    - 0.9
  local_crops_scale:
    - 0.3
    - 0.5
  num_global_crops: 2
  num_local_crops: 5

  # MLP hparams
  mlp_out_dim: 4096
  mlp_use_bn: False
  mlp_norm_last_layer: True
  mlp_nlayers: 3
  mlp_hidden_dim: 2048
  mlp_bottleneck_dim: 256

  # Training hparams Dino
  warmup_teacher_temp: 0.01 # Initial value
  teacher_temp: 0.01 # Final value after warmup
  warmup_teacher_temp_epochs: 0 
  student_temp: 0.1
  center_momentum: 0.9
  teacher_momentum: 0.9995 # Was 0.996
  pooling: "cls" # "cls" or "average"

  # Training hparams General
  anneal_lr: 1
  anneal_per_step: True
  warmup_duration: 100000
  lr_start: 0
  blr: 1.0e-5
  lr_end: 1.0e-7
  batch_size: 64
  ref_batch_size: 256
  weight_decay: 0.04
  epochs: 2
  subset: 0
  num_workers: 0
  dropout: 0